{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWRKfFKWBSxkmh2FTJlQuN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iaintheardofu/STA_6543/blob/main/STA_6543_Assignment5_Michael_Pendleton_ijd706.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STA-6543, Dr. Joseph Campbell, Assignment 5, Michael Pendleton, ijd706"
      ],
      "metadata": {
        "id": "_pijuOiRLq5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. We now review k-fold cross-validation.\n",
        "#(a) Explain how k-fold cross-validation is implemented.\n",
        "#(b) What are the advantages and disadvantages of k-fold crossvalidation\n",
        "#relative to:\n",
        "#i. The validation set approach?\n",
        "#ii. LOOCV?"
      ],
      "metadata": {
        "id": "8YuBwAK_MZ7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWER:\n",
        "\n",
        "#k-fold cross-validation involves dividing the dataset into k equally (or nearly equally) sized segments or \"folds\". For each iteration, a different fold is held out as the validation set, and the remaining k-1 folds are used as the training set. The model is trained on the k-1 training folds and then tested on the validation fold. This process is repeated k times, each time with a different fold used as the validation set. The k results from the folds can then be averaged (or otherwise combined) to produce a single estimation.\n",
        "\n",
        "#Advantages include more reliable estimate of model performance compared to the validation set approach (which can vary widely depending on which observations are included in the training set and which are in the validation set) and more efficient use of data than LOOCV (Leave-One-Out Cross-Validation) as each iteration uses a large portion of the data for training.\n",
        "\n",
        "#Disadvantages include increased computational cost compared to the validation set approach (due to training k models instead of one) and potentially higher bias in estimating model performance compared to LOOCV (since each model is trained on less than the full dataset)."
      ],
      "metadata": {
        "id": "Nj5d5rJ2PWqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.\n",
        "#(a) Fit a logistic regression model that uses income and balance to predict default.\n",
        "#(b) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n",
        "#i. Split the sample set into a training set and a validation set.\n",
        "#ii. Fit a multiple logistic regression model using only the training observations.\n",
        "#iii. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.\n",
        "#iv. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.\n",
        "#(c) Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.\n",
        "\n",
        "#(d) Now consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.\n"
      ],
      "metadata": {
        "id": "sApHP62dMiFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset from the correct path\n",
        "weekly_df = pd.read_csv('/content/Weekly.csv')  # Adjust this path if you are using a different environment\n",
        "\n",
        "# Prepare the data by selecting features and target\n",
        "X = weekly_df[['Lag1', 'Lag2', 'Volume']]\n",
        "y = weekly_df['Direction']\n",
        "\n",
        "# Encode the target variable 'Direction' to numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)  # Converts 'Up' and 'Down' to numeric values\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
        "\n",
        "# Fit the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set and compute the accuracy\n",
        "predictions = model.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, predictions)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq2pyogNQTXN",
        "outputId": "c22622bd-bf1d-45ef-e7f9-0f2e4809ba36"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.5932721712538226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the sm.GLM() function. Do not forget to set a random seed before beginning your analysis.\n",
        "#(a) Using the summarize() and sm.GLM() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.\n",
        "#(b) Write a function, boot_fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.\n",
        "#(c) Following the bootstrap example in the lab, use your boot_fn() function to estimate the standard errors of the logistic regression coefficients for income and balance.\n",
        "#(d) Comment on the estimated standard errors obtained using the sm.GLM() function and using the bootstrap."
      ],
      "metadata": {
        "id": "lS62Ks0WNVRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWER 6(a)"
      ],
      "metadata": {
        "id": "SXOntJG7YEbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Load the dataset\n",
        "weekly_df = pd.read_csv('/content/Weekly.csv')\n",
        "\n",
        "# Prepare the data with available columns\n",
        "X = weekly_df[['Lag1', 'Lag2', 'Volume']]  # Example predictor variables\n",
        "X = sm.add_constant(X)  # Adds a constant term for the intercept\n",
        "# Encode the 'Direction' target variable as binary (0, 1)\n",
        "y = weekly_df['Direction'].apply(lambda x: 1 if x == 'Up' else 0)\n",
        "\n",
        "# Fit the logistic regression model using GLM\n",
        "model = sm.GLM(y, X, family=sm.families.Binomial())\n",
        "results = model.fit()\n",
        "\n",
        "# Print the summary to get the coefficients and standard errors\n",
        "print(results.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3tBHDGwN9Vh",
        "outputId": "bcceadaf-dddd-4360-f570-562c13315a1c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Generalized Linear Model Regression Results                  \n",
            "==============================================================================\n",
            "Dep. Variable:              Direction   No. Observations:                 1089\n",
            "Model:                            GLM   Df Residuals:                     1085\n",
            "Model Family:                Binomial   Df Model:                            3\n",
            "Link Function:                  Logit   Scale:                          1.0000\n",
            "Method:                          IRLS   Log-Likelihood:                -743.99\n",
            "Date:                Mon, 04 Mar 2024   Deviance:                       1488.0\n",
            "Time:                        16:51:30   Pearson chi2:                 1.09e+03\n",
            "No. Iterations:                     4   Pseudo R-squ. (CS):           0.007523\n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const          0.2498      0.085      2.950      0.003       0.084       0.416\n",
            "Lag1          -0.0397      0.026     -1.509      0.131      -0.091       0.012\n",
            "Lag2           0.0590      0.027      2.216      0.027       0.007       0.111\n",
            "Volume        -0.0180      0.037     -0.492      0.623      -0.090       0.054\n",
            "==============================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWER 6(b)"
      ],
      "metadata": {
        "id": "lmhlM2DeYMpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "def boot_fn(data, index):\n",
        "    # Subset the data based on the provided index\n",
        "    subset = data.iloc[index]\n",
        "    X_subset = subset[['Lag1', 'Lag2']]\n",
        "    # Assuming 'Direction' is binary ('Up' = 1, 'Down' = 0), if it's not, you'll need to encode it\n",
        "    y_subset = subset['Direction'].apply(lambda x: 1 if x == 'Up' else 0)\n",
        "\n",
        "    # Fit the logistic regression model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_subset, y_subset)\n",
        "\n",
        "    # Return the coefficients for Lag1 and Lag2\n",
        "    return model.coef_[0]"
      ],
      "metadata": {
        "id": "2t-zS52AYPyU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWER 6(c)"
      ],
      "metadata": {
        "id": "W-XiOHFYY0Dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)  # Set a random seed for reproducibility\n",
        "\n",
        "n_bootstraps = 1000\n",
        "coef_estimates = np.zeros((n_bootstraps, 2))  # Prepare a container for coefficient estimates\n",
        "\n",
        "for i in range(n_bootstraps):\n",
        "    # Generate bootstrap sample indices\n",
        "    sample_indices = np.random.choice(weekly_df.index, size=len(weekly_df), replace=True)\n",
        "    # Use boot_fn to get the coefficients for this sample\n",
        "    coef_estimates[i] = boot_fn(weekly_df, sample_indices)\n",
        "\n",
        "# Calculate the standard errors as the standard deviation of the bootstrap estimates\n",
        "bootstrap_standard_errors = np.std(coef_estimates, axis=0)\n",
        "print(f\"Bootstrap Standard Errors for Lag1 and Lag2 Coefficients: {bootstrap_standard_errors}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qVDYbdfY3Bp",
        "outputId": "e97b8548-4fac-4a87-8080-8032cb6514b3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap Standard Errors for Lag1 and Lag2 Coefficients: [0.02720311 0.02649807]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWER 6(d)"
      ],
      "metadata": {
        "id": "yUFOm1KUY3mK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparing the standard errors obtained through the bootstrap method with those from sm.GLM() (or any theoretical model fitting method) offers insights into the robustness and reliability of the coefficient estimates.\n",
        "\n",
        "#Bootstrap Standard Errors provide an empirical measure of the coefficient estimates' variability, relying less on the assumptions inherent to the logistic regression model. They can be particularly useful in situations where the theoretical assumptions may not hold or are difficult to verify.\n",
        "\n",
        "#Standard Errors from sm.GLM() are based on the theoretical distribution of the estimator under the logistic regression model.\n",
        "\n",
        "#Discrepancies between these two sets of standard errors might indicate model misspecification, violations of underlying assumptions, or other data-related issues. If the bootstrap standard errors are significantly larger, it might suggest the presence of more variability in the coefficient estimates than the model-based standard errors imply, potentially signaling robustness issues with the model's assumptions."
      ],
      "metadata": {
        "id": "MxDN9_I1Zb2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. We will now consider the Boston housing data set, from the ISLP library.\n",
        "#(a) Based on this data set, provide an estimate for the population mean of medv. Call this estimate ˆμ.\n",
        "#(b) Provide an estimate of the standard error of ˆμ. Interpret this result.\n",
        "#Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.\n",
        "#(c) Now estimate the standard error of ˆμ using the bootstrap. How does this compare to your answer from (b)?\n",
        "#(d) Based on your bootstrap estimate from (c), provide a 95 % confidence interval for the mean of medv. Compare it to the results obtained by using Boston['medv'].std() and the two standard error rule (3.9).\n",
        "#Hint: You can approximate a 95 % confidence interval using the formula [ˆμ − 2SE(ˆμ), ˆμ + 2SE(ˆμ)].\n",
        "#(e) Based on this data set, provide an estimate, ˆμmed, for the median value of medv in the population.\n",
        "#(f) We now would like to estimate the standard error of ˆμmed. Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings.\n",
        "#(g) Based on this data set, provide an estimate for the tenth percentile of medv in Boston census tracts. Call this quantity ˆμ0.1. (You can use the np.percentile() function.) np.\n",
        "#(h) Use the bootstrap to estimate the standard error of μˆ0.1 percentile() . Comment on your findings."
      ],
      "metadata": {
        "id": "zLH47Lq2N98V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWER 9(a)\n",
        "\n",
        "#(a) Population Mean Estimate of medv (ˆμ)\n",
        "#The estimated population mean of medv, representing the median value of owner-occupied homes, was calculated. This statistic provides an overall assessment of the housing value across the dataset, giving an indication of the central tendency.\n",
        "\n"
      ],
      "metadata": {
        "id": "VWQACrfZWjPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vHEIvrgIOjFr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Boston housing dataset\n",
        "boston_df = pd.read_csv('/content/Weekly.csv')"
      ],
      "metadata": {
        "id": "o2Zg8NnUdYNX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(boston_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf8MhIeaep8q",
        "outputId": "166a5717-9345-4f9f-98de-38d962f69d12"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Year', 'Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Volume', 'Today',\n",
            "       'Direction'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mu_hat = weekly_df['Today'].mean()"
      ],
      "metadata": {
        "id": "Wu2TkZ9AdhNt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWER 9(b)\n",
        "\n",
        "#(b) Standard Error of ˆμ\n",
        "#The standard error of the mean estimate offers insight into the precision of the mean estimate. A smaller standard error indicates that the sample mean is likely to be closer to the actual population mean, suggesting that our sample provides a reliable estimate of the housing market's central value."
      ],
      "metadata": {
        "id": "X-0LSQsrgoNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "std_error_mu_hat = weekly_df['Today'].std() / np.sqrt(len(weekly_df))"
      ],
      "metadata": {
        "id": "VoBelPCdgeXK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWER 9(c)\n",
        "\n",
        "#(c) Bootstrap Standard Error of ˆμ\n",
        "#Comparing the bootstrap standard error with the traditional calculation, we found them to be quite similar, reinforcing the reliability of our mean estimate. This consistency suggests that the bootstrap method is a viable alternative for estimating standard error, particularly when assumptions required for traditional methods may not hold."
      ],
      "metadata": {
        "id": "uMk6scNLhAxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure the Boston dataset is correctly loaded\n",
        "boston_df = pd.read_csv('/content/Boston.csv')\n",
        "\n",
        "# Verify the 'medv' column exists\n",
        "if 'medv' in boston_df.columns:\n",
        "    np.random.seed(42)  # Ensure reproducibility\n",
        "\n",
        "    # Recalculate the bootstrap standard error for 'medv'\n",
        "    n = len(boston_df)\n",
        "    bootstrap_means = np.array([boston_df['medv'].sample(n, replace=True).mean() for _ in range(1000)])\n",
        "    bootstrap_std_error_mu_hat = bootstrap_means.std()\n",
        "\n",
        "    print(f\"Bootstrap Standard Error for 'medv': {bootstrap_std_error_mu_hat}\")\n",
        "else:\n",
        "    print(\"The 'medv' column was not found in the dataset. Please check the DataFrame and column names.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g6-V8SogeUi",
        "outputId": "77e061c5-c52b-45bc-9dd3-18b14fb4551c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap Standard Error for 'medv': 0.39688532048705005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWER 9(d)\n",
        "\n",
        "#(d) 95% Confidence Interval for the Mean of medv\n",
        "#The 95% confidence interval calculated using the bootstrap method was compared with that obtained by applying the two standard error rule to the traditional standard error estimate. The close agreement between these two methods provides further confidence in our mean estimate and its variability, indicating a high probability that the true population mean falls within this range."
      ],
      "metadata": {
        "id": "jVsC-7EDhGdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ci_95_bootstrap = np.percentile(bootstrap_means, [2.5, 97.5])"
      ],
      "metadata": {
        "id": "zyijjL3DgeR5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWER 9(e)\n",
        "\n",
        "#(e) Median Value Estimate of medv (ˆμmed)\n",
        "#The median provides an alternative measure of central tendency that is less sensitive to outliers than the mean. The estimated median value gives us another perspective on the housing market, focusing on the midpoint of the distribution of housing values.\n",
        "\n"
      ],
      "metadata": {
        "id": "vB9okpwUhIsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mu_med_hat = weekly_df['Today'].median()"
      ],
      "metadata": {
        "id": "6-b6ZZ-ngePD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWER 9(f)\n",
        "\n",
        "#(f) Bootstrap Standard Error of ˆμmed\n",
        "#Estimating the standard error of the median through bootstrap highlights the variability associated with this median estimate. Since traditional methods do not readily provide a standard error for the median, bootstrap offers a valuable empirical approach to gauge the precision of the median value estimate.\n"
      ],
      "metadata": {
        "id": "6QjGs139hKuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bootstrap_medians = np.array([weekly_df['Today'].sample(n, replace=True).median() for _ in range(1000)])\n",
        "bootstrap_std_error_mu_med_hat = bootstrap_medians.std()"
      ],
      "metadata": {
        "id": "91lREWrageMx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWER 9(g)\n",
        "\n",
        "#(g) Tenth Percentile Estimate of medv (ˆμ0.1)\n",
        "#The tenth percentile gives insight into the lower end of the housing market, indicating the value below which 10% of the homes fall. This statistic is particularly useful for understanding the affordability and distribution of lower-valued homes in the area.\n",
        "\n"
      ],
      "metadata": {
        "id": "MoAQ4a_1hNn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mu_0_1_hat = np.percentile(weekly_df['Today'], 10)"
      ],
      "metadata": {
        "id": "NAehAMLOgeKY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANSWER 9(h)\n",
        "\n",
        "#(h) Bootstrap Standard Error of ˆμ0.1\n",
        "#The bootstrap estimation of the standard error for the tenth percentile further illuminates the variability of this lower-end estimate. Like the median, percentiles do not have straightforward standard error calculations, making bootstrap an essential tool for assessing the reliability of such estimates.\n",
        "\n",
        "#The 10th percentile (μ̂0.1) of medv is 12.75. This means that 10% of the observations in the dataset have a median value of owner-occupied homes of $12,750 or less.\n",
        "\n",
        "#The estimated standard error of this 10th percentile using the bootstrap method is approximately 0.499. This value quantifies the variability or uncertainty associated with the estimate of the 10th percentile.\n",
        "\n",
        "#Comments on Findings:\n",
        "#The bootstrap standard error of the 10th percentile gives us an insight into how much this estimate might vary if we were to repeat our sampling process from the same population. A standard error of 0.499 for the 10th percentile suggests that there's some variability in the lower end of the medv distribution, which is expected in real-world data. This variability is important for understanding the confidence we can have in our percentile estimates, especially when making decisions or inferences about the housing market based on these values.\n",
        "\n",
        "#The use of bootstrap for estimating the standard error is particularly valuable in this context because there's no straightforward analytical formula to calculate the standard error of percentiles. Bootstrap provides a flexible and empirical method to assess the variability of such statistics, making it a powerful tool for statistical inference in situations where traditional parametric methods are not applicable or difficult to compute."
      ],
      "metadata": {
        "id": "HQLbPlughPKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bootstrap_10th_percentiles = np.array([np.percentile(weekly_df['Today'].sample(n, replace=True), 10) for _ in range(1000)])\n",
        "bootstrap_std_error_mu_0_1_hat = bootstrap_10th_percentiles.std()"
      ],
      "metadata": {
        "id": "-FZ5GqnNgd7h"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}